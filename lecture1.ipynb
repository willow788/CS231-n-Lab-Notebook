{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq5AMu1EAo557DKjlNsarC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50e2f2d0",
        "outputId": "66b62705-fdd4-4b47-bee4-c6f89ffb0713"
      },
      "source": [
        "# Download the CIFAR-10 dataset\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "# Extract the dataset\n",
        "!tar -xzf cifar-10-python.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-09 01:41:29--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  85.4MB/s    in 1.9s    \n",
            "\n",
            "2026-01-09 01:41:31 (85.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qJU339FYb75",
        "outputId": "7ac84f02-daf7-456c-84c2-a1259f967a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3072)\n",
            "(50000,)\n",
            "(10000, 3072)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def loading_cifar_batch(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    data = pickle.load(f, encoding='bytes')\n",
        "    X= data[b'data'].astype(np.float32)\n",
        "    y= data[b'labels']\n",
        "    return X, y\n",
        "\n",
        "def load_cifar(root):\n",
        "  Xs, ys = [], []\n",
        "  for i in range(1, 6):\n",
        "    X, y = loading_cifar_batch(os.path.join(root, f'data_batch_{i}'))\n",
        "    Xs.append(X)\n",
        "    ys.append(y)\n",
        "  X_train = np.concatenate(Xs)\n",
        "  y_train = np.concatenate(ys)\n",
        "\n",
        "  X_test, y_test = loading_cifar_batch(os.path.join(root, 'test_batch'))\n",
        "  # Convert y_test to a numpy array, as loading_cifar_batch returns a list for labels\n",
        "  y_test = np.array(y_test)\n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_cifar('cifar-10-batches-py')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalising the data\n",
        "X_mean = np.mean(X_train, axis=0)\n",
        "X_train -= X_mean\n",
        "X_test -= X_mean\n",
        "print(X_train.shape)\n",
        "\n",
        "#now the imagewill not be botheered by light brithness and all those things\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHc20Hc6c5_i",
        "outputId": "09b83ecd-50b7-4063-bb8e-90a149d47c48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 3072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = 1000    # Number of samples to use for the validation set\n",
        "num_train_samples = X_train.shape[0] - num_val_samples # Adjust num_train_samples to ensure validation set is not empty\n",
        "\n",
        "# Ensure X_train and y_train are the full datasets loaded from cifar before slicing\n",
        "# (Assuming X_train and y_train refer to the full loaded data here)\n",
        "\n",
        "# Create the new training sets\n",
        "X_train_new = X_train[:num_train_samples]\n",
        "y_train_new = y_train[:num_train_samples]\n",
        "\n",
        "# Create the validation sets from the original data, immediately following the training samples\n",
        "X_val_new = X_train[num_train_samples : num_train_samples + num_val_samples]\n",
        "y_val_new = y_train[num_train_samples : num_train_samples + num_val_samples]\n",
        "\n",
        "# Reassign to the original variable names\n",
        "X_train = X_train_new\n",
        "y_train = y_train_new\n",
        "X_val = X_val_new\n",
        "y_val = y_val_new\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3BEq8GPfV6B",
        "outputId": "36796bbe-88b4-49d0-dfcb-75ae9229d5c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 3072)\n",
            "(3000,)\n",
            "(1000, 3072)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class linearClassifier:\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "\n",
        "\n",
        "  def train(self, X, y, lr= 1e-7,reg=1e-5,num_iters=100,batch_size=200,verbose=False):\n",
        "    num_train, dim = X.shape\n",
        "    num_classes = np.max(y) + 1\n",
        "\n",
        "    if self.w is None:\n",
        "      self.w = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "      batch_index = np.random.choice(num_train, batch_size)\n",
        "      X_batch = X[batch_index]\n",
        "      y_batch = y[batch_index]\n",
        "\n",
        "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      self.w -= lr * grad\n",
        "\n",
        "      if i % 10==0:\n",
        "        print(f\"iter {i}: loss {loss}\")\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    scores = X.dot(self.w)\n",
        "    return np.argmax(scores, axis=1)\n",
        "\n",
        "class linearSVM(linearClassifier):\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    num_train = X_batch.shape[0]\n",
        "    scores = X_batch.dot(self.w)\n",
        "    # Subtract the maximum score for numerical stability\n",
        "    scores -= np.max(scores, axis=1, keepdims=True)\n",
        "\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    # Calculate the cross-entropy loss\n",
        "    correct_log_probs = -np.log(probs[np.arange(num_train), y_batch])\n",
        "    data_loss = np.sum(correct_log_probs) / num_train\n",
        "\n",
        "    # Add regularization\n",
        "    reg_loss = 0.5 * reg * np.sum(self.w * self.w)\n",
        "    loss = data_loss + reg_loss\n",
        "\n",
        "    # Compute gradient\n",
        "    dscores = probs\n",
        "    dscores[np.arange(num_train), y_batch] -= 1\n",
        "    dscores /= num_train\n",
        "\n",
        "    dW = X_batch.T.dot(dscores)\n",
        "    # Add regularization gradient\n",
        "    dW += reg * self.w\n",
        "\n",
        "    return loss, dW"
      ],
      "metadata": {
        "id": "XGuEu0xhgOi0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = linearSVM()\n",
        "loss_history = model.train(X_train, y_train, lr=1e-7, reg=5e4, num_iters=1500, batch_size=200, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5XtHIhwuQcu",
        "outputId": "83f6e69b-b629-4be7-af5c-ae018c588b01"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 775.0970989241519\n",
            "iter 10: loss 701.0219135503994\n",
            "iter 20: loss 634.1342211255068\n",
            "iter 30: loss 573.407182493447\n",
            "iter 40: loss 518.5520741289662\n",
            "iter 50: loss 469.2123482186666\n",
            "iter 60: loss 424.31931410960476\n",
            "iter 70: loss 383.8343595746399\n",
            "iter 80: loss 347.4248896428685\n",
            "iter 90: loss 314.5128688490752\n",
            "iter 100: loss 284.63106131385024\n",
            "iter 110: loss 257.5595876163201\n",
            "iter 120: loss 233.1795352312717\n",
            "iter 130: loss 210.96883351703585\n",
            "iter 140: loss 190.84680380983087\n",
            "iter 150: loss 172.96393895186696\n",
            "iter 160: loss 156.61242688760407\n",
            "iter 170: loss 141.66264320252787\n",
            "iter 180: loss 128.4450359287655\n",
            "iter 190: loss 116.34794899555077\n",
            "iter 200: loss 105.35167424948031\n",
            "iter 210: loss 95.43021016603355\n",
            "iter 220: loss 86.61808654343875\n",
            "iter 230: loss 78.53848127293315\n",
            "iter 240: loss 71.14938076836309\n",
            "iter 250: loss 64.6113516794001\n",
            "iter 260: loss 58.68777288926101\n",
            "iter 270: loss 53.14943638904039\n",
            "iter 280: loss 48.359656580811176\n",
            "iter 290: loss 43.86459092022828\n",
            "iter 300: loss 39.87306615732912\n",
            "iter 310: loss 36.334481409775194\n",
            "iter 320: loss 33.078654268953294\n",
            "iter 330: loss 30.135528282402827\n",
            "iter 340: loss 27.436675742023855\n",
            "iter 350: loss 24.968168477374537\n",
            "iter 360: loss 22.804496727488704\n",
            "iter 370: loss 20.77567243992793\n",
            "iter 380: loss 18.95252820751814\n",
            "iter 390: loss 17.348780080242356\n",
            "iter 400: loss 15.914656558165127\n",
            "iter 410: loss 14.57363329794438\n",
            "iter 420: loss 13.416311498366298\n",
            "iter 430: loss 12.363003386740322\n",
            "iter 440: loss 11.302781641744831\n",
            "iter 450: loss 10.49284245183799\n",
            "iter 460: loss 9.648319367558958\n",
            "iter 470: loss 8.883328045686671\n",
            "iter 480: loss 8.198867717110279\n",
            "iter 490: loss 7.5885444176076495\n",
            "iter 500: loss 7.135149830654576\n",
            "iter 510: loss 6.66188170742224\n",
            "iter 520: loss 6.281489257542857\n",
            "iter 530: loss 5.790723396752237\n",
            "iter 540: loss 5.461707727857363\n",
            "iter 550: loss 5.1398511949057895\n",
            "iter 560: loss 4.850323477389118\n",
            "iter 570: loss 4.540870226296047\n",
            "iter 580: loss 4.320475743620961\n",
            "iter 590: loss 4.121625020207593\n",
            "iter 600: loss 3.9021193994632632\n",
            "iter 610: loss 3.7504204249268014\n",
            "iter 620: loss 3.6125118241287693\n",
            "iter 630: loss 3.419020004561321\n",
            "iter 640: loss 3.3047440433407944\n",
            "iter 650: loss 3.171638189416906\n",
            "iter 660: loss 3.046013539156485\n",
            "iter 670: loss 2.9048994056398927\n",
            "iter 680: loss 2.917052142080666\n",
            "iter 690: loss 2.7983969448531165\n",
            "iter 700: loss 2.7661177676170667\n",
            "iter 710: loss 2.6670705271086605\n",
            "iter 720: loss 2.6118190805081234\n",
            "iter 730: loss 2.5304720474549107\n",
            "iter 740: loss 2.5268591474476505\n",
            "iter 750: loss 2.510623600764501\n",
            "iter 760: loss 2.388933259207908\n",
            "iter 770: loss 2.3704708977773983\n",
            "iter 780: loss 2.4134591307009483\n",
            "iter 790: loss 2.316711068588098\n",
            "iter 800: loss 2.3470035527377986\n",
            "iter 810: loss 2.2855115167724445\n",
            "iter 820: loss 2.2319601889796203\n",
            "iter 830: loss 2.1959610467572843\n",
            "iter 840: loss 2.251679038683691\n",
            "iter 850: loss 2.1832716841887283\n",
            "iter 860: loss 2.187893751225275\n",
            "iter 870: loss 2.1960853774031945\n",
            "iter 880: loss 2.19274617535864\n",
            "iter 890: loss 2.219173255528737\n",
            "iter 900: loss 2.1547066262931804\n",
            "iter 910: loss 2.168227245930464\n",
            "iter 920: loss 2.1286741047409032\n",
            "iter 930: loss 2.109483160664157\n",
            "iter 940: loss 2.1847903021683144\n",
            "iter 950: loss 2.145195568869278\n",
            "iter 960: loss 2.173381957770306\n",
            "iter 970: loss 2.088467847476899\n",
            "iter 980: loss 2.1557614123339714\n",
            "iter 990: loss 2.063379141540503\n",
            "iter 1000: loss 2.0743487733898847\n",
            "iter 1010: loss 2.0881804155758728\n",
            "iter 1020: loss 2.112334006226339\n",
            "iter 1030: loss 2.0890261621619084\n",
            "iter 1040: loss 2.076156864534136\n",
            "iter 1050: loss 2.0831509184204506\n",
            "iter 1060: loss 2.1240293572532036\n",
            "iter 1070: loss 2.1003124789206415\n",
            "iter 1080: loss 2.0021581633991237\n",
            "iter 1090: loss 2.0933935415070275\n",
            "iter 1100: loss 2.036425451241129\n",
            "iter 1110: loss 2.040010758497245\n",
            "iter 1120: loss 2.1077273130120293\n",
            "iter 1130: loss 2.0768043979964372\n",
            "iter 1140: loss 2.094067736036515\n",
            "iter 1150: loss 2.0799729858456164\n",
            "iter 1160: loss 2.083996863698424\n",
            "iter 1170: loss 2.035176056606218\n",
            "iter 1180: loss 2.095469560130423\n",
            "iter 1190: loss 2.0239175378448166\n",
            "iter 1200: loss 2.065601255984019\n",
            "iter 1210: loss 2.0590007624940885\n",
            "iter 1220: loss 2.150384011247222\n",
            "iter 1230: loss 2.0923103666925877\n",
            "iter 1240: loss 2.0174064111883725\n",
            "iter 1250: loss 2.0322320780647503\n",
            "iter 1260: loss 2.0573280476539413\n",
            "iter 1270: loss 2.066311898425938\n",
            "iter 1280: loss 2.1054857441807706\n",
            "iter 1290: loss 2.0274232426909\n",
            "iter 1300: loss 2.0518487925413362\n",
            "iter 1310: loss 2.0721622905567223\n",
            "iter 1320: loss 2.1237591141700696\n",
            "iter 1330: loss 2.092908792878947\n",
            "iter 1340: loss 1.9925287473310938\n",
            "iter 1350: loss 2.069034177385254\n",
            "iter 1360: loss 2.081054574087383\n",
            "iter 1370: loss 2.1328093008315414\n",
            "iter 1380: loss 2.0822012374929337\n",
            "iter 1390: loss 2.033946387191598\n",
            "iter 1400: loss 2.0853295410316215\n",
            "iter 1410: loss 2.061985989812294\n",
            "iter 1420: loss 2.0339960160265638\n",
            "iter 1430: loss 2.064550695181552\n",
            "iter 1440: loss 2.0638197834796657\n",
            "iter 1450: loss 2.0556468515348088\n",
            "iter 1460: loss 2.0186035905145663\n",
            "iter 1470: loss 2.1088002676865734\n",
            "iter 1480: loss 2.063564519345568\n",
            "iter 1490: loss 2.064274560835891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(X_val)\n",
        "print(f\"Validation accuracy: {np.mean(y_val == y_val_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlCE3QhOur5v",
        "outputId": "b685a946-6324-49e4-82bb-65d7340d58df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class softmaxClassifier(linearClassifier):\n",
        "  def loss(self, X, y, reg):\n",
        "    loss = 0.0\n",
        "    dw = np.zeros_like(self.w).astype(np.float32)\n",
        "\n",
        "    num_train = X.shape[0]\n",
        "    num_classes = self.w.shape[1]\n",
        "\n",
        "    for i in range(num_train):\n",
        "      scores = X[i].dot(self.w)\n",
        "      scores -= np.max(scores)\n",
        "\n",
        "      exp_scores = np.exp(scores)\n",
        "      probability = exp_scores/np.sum(exp_scores)\n",
        "\n",
        "      loss += -np.log(probability[y[i]])\n",
        "\n",
        "      for j in range(num_classes):\n",
        "        if j == y[i]:\n",
        "          dw[:, j] += (probability[j] - 1) * X[i]\n",
        "        else:\n",
        "          dw[:, j] += probability[j] * X[i]\n",
        "\n",
        "    # These lines were inside the loop and should be outside to average over the entire batch\n",
        "    # and apply regularization correctly once per batch.\n",
        "    loss /= num_train\n",
        "    dw /= num_train\n",
        "\n",
        "    loss += reg * np.sum(self.w * self.w)\n",
        "    dw += 2 * reg * self.w\n",
        "\n",
        "    return loss, dw"
      ],
      "metadata": {
        "id": "CwU3c_Mk0agp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = softmaxClassifier()\n",
        "loss_history = softmax.train(X_train, y_train, lr=1e-7, reg=5e4, num_iters=1500, batch_size=200, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65akZ_k17F_",
        "outputId": "5a604f6c-d9f8-431b-b986-a43fe4ef6a01"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 1575.6257915357094\n",
            "iter 10: loss 1287.9212252918637\n",
            "iter 20: loss 1053.3753391028335\n",
            "iter 30: loss 861.3694242165944\n",
            "iter 40: loss 704.448719676872\n",
            "iter 50: loss 576.3508056424245\n",
            "iter 60: loss 471.66359149767885\n",
            "iter 70: loss 385.82651621519807\n",
            "iter 80: loss 315.99015231622604\n",
            "iter 90: loss 258.73830442328307\n",
            "iter 100: loss 211.9441983748102\n",
            "iter 110: loss 173.63978775053457\n",
            "iter 120: loss 142.40786856751657\n",
            "iter 130: loss 116.82068294503901\n",
            "iter 140: loss 95.93801789853057\n",
            "iter 150: loss 78.85947928358686\n",
            "iter 160: loss 64.81622401628792\n",
            "iter 170: loss 53.387334841138696\n",
            "iter 180: loss 43.95321489019472\n",
            "iter 190: loss 36.38914532865286\n",
            "iter 200: loss 30.14376292874821\n",
            "iter 210: loss 25.001520265581785\n",
            "iter 220: loss 20.90078260446838\n",
            "iter 230: loss 17.44998342052756\n",
            "iter 240: loss 14.596508470401814\n",
            "iter 250: loss 12.358814951994525\n",
            "iter 260: loss 10.466382538935859\n",
            "iter 270: loss 9.004395805924158\n",
            "iter 280: loss 7.730209356110397\n",
            "iter 290: loss 6.688411257264583\n",
            "iter 300: loss 5.868248608002265\n",
            "iter 310: loss 5.1315548784273695\n",
            "iter 320: loss 4.650837846012106\n",
            "iter 330: loss 4.137178131420887\n",
            "iter 340: loss 3.8125889710820937\n",
            "iter 350: loss 3.453943027646205\n",
            "iter 360: loss 3.2666674782115983\n",
            "iter 370: loss 3.0216003731408057\n",
            "iter 380: loss 2.819549704303638\n",
            "iter 390: loss 2.7542605106188924\n",
            "iter 400: loss 2.5888541497266906\n",
            "iter 410: loss 2.473682906077765\n",
            "iter 420: loss 2.450840273386731\n",
            "iter 430: loss 2.3846532481613307\n",
            "iter 440: loss 2.343103678695957\n",
            "iter 450: loss 2.295301849578948\n",
            "iter 460: loss 2.2649010372738587\n",
            "iter 470: loss 2.238746217311099\n",
            "iter 480: loss 2.2268446443099337\n",
            "iter 490: loss 2.213863238949439\n",
            "iter 500: loss 2.2093384568299057\n",
            "iter 510: loss 2.1564449274451514\n",
            "iter 520: loss 2.1890357260207853\n",
            "iter 530: loss 2.2094105566072617\n",
            "iter 540: loss 2.145404887385021\n",
            "iter 550: loss 2.136530794649602\n",
            "iter 560: loss 2.181105684464569\n",
            "iter 570: loss 2.1122514897491227\n",
            "iter 580: loss 2.121788735310789\n",
            "iter 590: loss 2.13938483165145\n",
            "iter 600: loss 2.100319169489587\n",
            "iter 610: loss 2.111491244827242\n",
            "iter 620: loss 2.152165049860547\n",
            "iter 630: loss 2.1368515406345225\n",
            "iter 640: loss 2.14295256117608\n",
            "iter 650: loss 2.147208001230139\n",
            "iter 660: loss 2.102470342317351\n",
            "iter 670: loss 2.124429961968192\n",
            "iter 680: loss 2.1288618136082076\n",
            "iter 690: loss 2.085746500855866\n",
            "iter 700: loss 2.1124431809148283\n",
            "iter 710: loss 2.1138492004359843\n",
            "iter 720: loss 2.166991625347363\n",
            "iter 730: loss 2.1844133789818807\n",
            "iter 740: loss 2.1116995870961426\n",
            "iter 750: loss 2.181663595640308\n",
            "iter 760: loss 2.1442361067708386\n",
            "iter 770: loss 2.116496484764727\n",
            "iter 780: loss 2.1457514725668383\n",
            "iter 790: loss 2.0942881272672556\n",
            "iter 800: loss 2.046066532084278\n",
            "iter 810: loss 2.159788993650428\n",
            "iter 820: loss 2.1685052627276384\n",
            "iter 830: loss 2.0959911655288246\n",
            "iter 840: loss 2.0571515339467954\n",
            "iter 850: loss 2.144825074659812\n",
            "iter 860: loss 2.0963098898419936\n",
            "iter 870: loss 2.0668895939432184\n",
            "iter 880: loss 2.102698069577105\n",
            "iter 890: loss 2.1335371935636473\n",
            "iter 900: loss 2.1099201346458725\n",
            "iter 910: loss 2.1024065609066986\n",
            "iter 920: loss 2.0755224762029867\n",
            "iter 930: loss 2.13085698687469\n",
            "iter 940: loss 2.0886244640684923\n",
            "iter 950: loss 2.1171662107342426\n",
            "iter 960: loss 2.1217648812376395\n",
            "iter 970: loss 2.133979762713059\n",
            "iter 980: loss 2.1316140460636754\n",
            "iter 990: loss 2.0904226823106713\n",
            "iter 1000: loss 2.134171072577806\n",
            "iter 1010: loss 2.127048317563506\n",
            "iter 1020: loss 2.1638449033685903\n",
            "iter 1030: loss 2.118394451423045\n",
            "iter 1040: loss 2.1135381556414474\n",
            "iter 1050: loss 2.122867114750072\n",
            "iter 1060: loss 2.165094880065234\n",
            "iter 1070: loss 2.116007047869071\n",
            "iter 1080: loss 2.095986813155783\n",
            "iter 1090: loss 2.1232461740026514\n",
            "iter 1100: loss 2.190314944678072\n",
            "iter 1110: loss 2.1099033186169622\n",
            "iter 1120: loss 2.154164875090979\n",
            "iter 1130: loss 2.1572987263945915\n",
            "iter 1140: loss 2.147678676453223\n",
            "iter 1150: loss 2.1050304585240776\n",
            "iter 1160: loss 2.1328116241739896\n",
            "iter 1170: loss 2.1108523123948895\n",
            "iter 1180: loss 2.1326039155547294\n",
            "iter 1190: loss 2.1365698630031087\n",
            "iter 1200: loss 2.1529496129490893\n",
            "iter 1210: loss 2.115575323796626\n",
            "iter 1220: loss 2.128863482880331\n",
            "iter 1230: loss 2.129594138922995\n",
            "iter 1240: loss 2.094269903844774\n",
            "iter 1250: loss 2.16110495417328\n",
            "iter 1260: loss 2.121112336442655\n",
            "iter 1270: loss 2.134113645591264\n",
            "iter 1280: loss 2.1578876485781437\n",
            "iter 1290: loss 2.163481492760785\n",
            "iter 1300: loss 2.156684092119124\n",
            "iter 1310: loss 2.1156827827189746\n",
            "iter 1320: loss 2.1708921330851876\n",
            "iter 1330: loss 2.1274638158316246\n",
            "iter 1340: loss 2.1029662115520624\n",
            "iter 1350: loss 2.0839157159715627\n",
            "iter 1360: loss 2.1249876245835453\n",
            "iter 1370: loss 2.0789497135514012\n",
            "iter 1380: loss 2.1029386374692627\n",
            "iter 1390: loss 2.0714707514938735\n",
            "iter 1400: loss 2.1112259539397384\n",
            "iter 1410: loss 2.1046547059872895\n",
            "iter 1420: loss 2.144869635135182\n",
            "iter 1430: loss 2.132503276994647\n",
            "iter 1440: loss 2.18351942858346\n",
            "iter 1450: loss 2.128399866276204\n",
            "iter 1460: loss 2.120763897175365\n",
            "iter 1470: loss 2.028285026744366\n",
            "iter 1480: loss 2.18350839582614\n",
            "iter 1490: loss 2.1103374249589146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = softmax.predict(X_val)\n",
        "answer = np.mean(y_val == y_val_pred)\n",
        "percentage = answer * 100\n",
        "print(f\"Validation accuracy: {percentage} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IAJHeof2rb_",
        "outputId": "f787fa7d-329b-475c-de3b-5f5dd6cd7dc4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 30.2 %\n"
          ]
        }
      ]
    }
  ]
}