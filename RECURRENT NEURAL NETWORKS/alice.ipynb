{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb04ec2b",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Alice in Wonderland Text Generation\n",
    "\n",
    "This notebook uses LSTM neural networks with CUDA GPU acceleration to generate text in the style of \"Alice in Wonderland\". \n",
    "\n",
    "**Important:** Restart the kernel and run cells 1-2 first to verify GPU detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc683b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available: True\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory: 4.29 GB\n",
      "\n",
      "Note: TensorFlow running in CPU mode on Windows.\n",
      "For GPU with TensorFlow on Windows, download CUDA Toolkit from NVIDIA.\n",
      "PyTorch is already GPU-enabled on this system!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RNN\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# TensorFlow CPU mode (Windows CUDA setup is complex, PyTorch is preferred)\n",
    "print(\"\\nNote: TensorFlow running in CPU mode on Windows.\")\n",
    "print(\"For GPU with TensorFlow on Windows, download CUDA Toolkit from NVIDIA.\")\n",
    "print(\"PyTorch is already GPU-enabled on this system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9f9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available: True\n",
      "TensorFlow GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability for both PyTorch and TensorFlow\n",
    "import torch\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"TensorFlow GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(f\"TensorFlow will use: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205f7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfb550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the book\n",
    "text = (open(\"alice_in_wonderland.txt\").read())\n",
    "text = text.lower()\n",
    "#here we are converiing the text into its lower form as in changing the capitals to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed044d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new vocabulary size is: 44\n",
      "the unique characters are: [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#creating character mappig\n",
    "\"\"\"here we are using character mapping to assign an number to a character or a word \n",
    "basic idea: here we are mapping a catagorical value to a numberical value \n",
    "why? becauese maching understand numbers beter than the characters\n",
    "this is also the first step in nlp tasks and also very pivotal in an rnn project\n",
    "\"\"\"\n",
    "\n",
    "#1st we we define characters as a list \n",
    "characters = sorted(list(set(text)))\n",
    "#here characters is a list of all the unique characters in the text\n",
    "\n",
    "num_to_char = {n:char for n, char in enumerate(characters)}\n",
    "#here num_to_char is a dictionary the maps the characters to numbers    \n",
    "\n",
    "char_to_num = {char: n for n, char in enumerate(characters)}\n",
    "#here char_to_num is a dictionary that maps numbers to characters reverse mapping ig\n",
    "\n",
    "#defining the vocabulary size\n",
    "#vocalb_size = len(characters)\n",
    "\n",
    "#print(f\"print the number of unique characters: {vocalb_size}\")\n",
    "#print(f\"the unique characters are: {characters}\")\n",
    "\n",
    "#we will remov * and \\n from the characters as they are not useful in our model training\n",
    "characters.remove('*')\n",
    "characters.remove('\\n')\n",
    "vocalb_size = len(characters)\n",
    "print(f\"the new vocabulary size is: {vocalb_size}\")\n",
    "print(f\"the unique characters are: {characters}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d417b545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the input sequences is: 148474\n"
     ]
    }
   ],
   "source": [
    "#shape fixing foor training the model\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "#creating the sequences and the labels\n",
    "#here length is the total length of the text\n",
    "#seq_length is the length of each sequence\n",
    "\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "\n",
    "##looping through the text to create sequences and labels\n",
    "for i in range(0, length - seq_length, 1):\n",
    "    #here we are creating sequences of length seq_length thus i + seq_length\n",
    "    sequence = text[i:i + seq_length]\n",
    "    #here text[i:i + seq_length] gives us the sequence from position i to i + seq_length\n",
    "\n",
    "    #here we are creating the label for each sequence so we take text at position of the sequence and tag it as the label\n",
    "    label = text[i + seq_length] \n",
    "\n",
    "    #now we are appending the sequences and labels to the x and y lists after converting them to their numerical form using the char_to_num dictionary   \n",
    "    x.append([char_to_num[char] for char in sequence])\n",
    "    y.append(char_to_num[label])\n",
    "\n",
    "#priniting the length of the input sequences\n",
    "print(f\"the length of the input sequences is: {len(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b11c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now modifiying them so that they can be used to train the model\n",
    "x_mod = np.reshape(x, (len(x), seq_length, 1))\n",
    "#here we are reshaping the x to be of the shape (number of sequences, sequence length, 1)   \n",
    "x_mod = x_mod / float(vocalb_size)\n",
    "#here we are normalizing the x_mod by dividing it by the vocabulary size\n",
    "y_modified = to_categorical(y)\n",
    "#here we are converting the y to categorical form using one hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52653d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model created on GPU: True\n",
      "Model architecture:\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(1, 700, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=700, out_features=46, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#main training the model - PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_tensor = torch.FloatTensor(x_mod).to(device)\n",
    "y_tensor = torch.LongTensor(np.argmax(y_modified, axis=1)).to(device)\n",
    "\n",
    "# Define PyTorch LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the last output for classification\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_out)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(input_size=1, hidden_size=700, num_layers=3, num_classes=y_modified.shape[1]).to(device)\n",
    "print(f\"Model created on GPU: {next(model.parameters()).is_cuda}\")\n",
    "print(f\"Model architecture:\\n{model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87954b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup training parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Store losses for monitoring\n",
    "train_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c2d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on GPU...\n",
      "\n",
      "============================================================\n",
      "Epoch    Loss         Batch Progress       Time      \n",
      "============================================================\n",
      "1        0.3710       ███████████████ 100.0%  204.81s\n",
      "2        0.3497       ███████████████ 100.0%  205.27s\n",
      "3        0.3314       ███████████████ 100.0%  205.77s\n",
      "4        0.3128       ███████████████ 100.0%  206.06s\n",
      "5        0.3070       ███████████████ 100.0%  204.25s\n",
      "6        0.2931       ███████████████ 100.0%  204.09s\n",
      "7        0.2868       ███████████████ 100.0%  200.36s\n",
      "8        0.2742       ███████████████ 100.0%  204.40s\n",
      "9        0.2717       ███████████████ 100.0%  203.83s\n",
      "10       0.8698       ███████████████ 100.0%  205.05s\n",
      "11       2.9746       █████████░░░░░░  60.0%\r"
     ]
    }
   ],
   "source": [
    "#training the model on GPU\n",
    "import time\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "# Create data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training on GPU...\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Epoch':<8} {'Loss':<12} {'Batch Progress':<20} {'Time':<10}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = (batch_idx + 1) / len(dataloader)\n",
    "        bar_length = 15\n",
    "        filled = int(bar_length * progress)\n",
    "        bar = '█' * filled + '░' * (bar_length - filled)\n",
    "        \n",
    "        if (batch_idx + 1) % max(1, len(dataloader) // 5) == 0:\n",
    "            print(f\"{epoch+1:<8} {epoch_loss/(batch_idx+1):<12.4f} {bar} {progress*100:>5.1f}%\", end='\\r')\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"{epoch+1:<8} {avg_loss:<12.4f} {'█' * 15} 100.0%  {epoch_time:>6.2f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(f\"Training complete! Total time: {total_time/60:.2f} minutes\")print(f\"Final loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda5580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training for 30 more epochs...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m loss = loss_fn(outputs, batch_y)\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     18\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "# Continue training for more epochs\n",
    "model.train()  # Ensure model is in training mode\n",
    "\n",
    "additional_epochs = 30\n",
    "print(f\"Continuing training for {additional_epochs} more epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        progress = (batch_idx + 1) / len(dataloader)\n",
    "        bar_length = 15\n",
    "        filled = int(bar_length * progress)\n",
    "        bar = '█' * filled + '░' * (bar_length - filled)\n",
    "        \n",
    "        if (batch_idx + 1) % max(1, len(dataloader) // 5) == 0:\n",
    "            print(f\"{epoch+1:<8} {epoch_loss/(batch_idx+1):<12.4f} {bar} {progress*100:>5.1f}%\", end='\\r')\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"{epoch+1:<8} {avg_loss:<12.4f} {'█' * 15} 100.0%  {epoch_time:>6.2f}s\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Training extended! Current loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862867bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "\n",
      "Generated text sample:\n",
      " !\"'(),-.03:;?[]_`abcdefghijklmnopqrstuvwxyz !\"'(),-.03:;?[]_`abcdefghijklmnopqrstuvwxyz !\"'(),-.03:!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!ejcrvgt!k  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "...\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "#generating text using the trained PyTorch model with temperature sampling\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create a generation-time mapping aligned with the current character list\n",
    "num_to_char_gen = {n: char for n, char in enumerate(characters)}\n",
    "char_to_num_gen = {char: n for n, char in enumerate(characters)}\n",
    "vocab_size_gen = len(characters)\n",
    "\n",
    "# Use a meaningful seed from the actual text (start of the book)\n",
    "seed_text = text[:seq_length]\n",
    "seed_sequence = [char_to_num_gen.get(char, 0) for char in seed_text if char in char_to_num_gen]\n",
    "\n",
    "# Pad if necessary\n",
    "while len(seed_sequence) < seq_length:\n",
    "    seed_sequence.append(0)\n",
    "seed_sequence = seed_sequence[:seq_length]\n",
    "\n",
    "generated_text = list(seed_text[:seq_length])\n",
    "\n",
    "print(\"Seed text:\")\n",
    "print(''.join(generated_text))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating text with temperature sampling...\\n\")\n",
    "\n",
    "# Temperature for sampling (lower = more conservative, higher = more random)\n",
    "temperature = 0.5\n",
    "\n",
    "for i in range(500):\n",
    "    # Prepare input\n",
    "    x_input = np.array(seed_sequence).reshape(1, seq_length, 1)\n",
    "    x_input = torch.FloatTensor(x_input / float(vocab_size_gen)).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_input)\n",
    "        # Apply temperature\n",
    "        pred = pred / temperature\n",
    "        # Get probabilities\n",
    "        probs = torch.softmax(pred, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Clip probabilities to valid vocabulary size\n",
    "        probs = probs[:vocab_size_gen]\n",
    "        probs = probs / probs.sum()  # Renormalize\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        pred_index = np.random.choice(len(probs), p=probs)\n",
    "    \n",
    "    # Ensure valid index\n",
    "    if pred_index >= vocab_size_gen:\n",
    "        pred_index = pred_index % vocab_size_gen\n",
    "    \n",
    "    # Append to generated text\n",
    "    generated_text.append(num_to_char_gen[pred_index])\n",
    "    \n",
    "    # Update seed\n",
    "    seed_sequence.append(pred_index)\n",
    "    seed_sequence = seed_sequence[1:]\n",
    "\n",
    "# Print generated text\n",
    "print(\"Generated text:\")\n",
    "print(''.join(generated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b0c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
